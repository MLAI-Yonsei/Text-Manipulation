{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"XHZLW17Xo45Y"},"outputs":[],"source":["import os\n","import json\n","import re\n","import pandas as pd\n","import numpy as np\n","from gensim import models\n","from Levenshtein import distance as levenshtein"]},{"cell_type":"markdown","metadata":{"id":"jDc2UrlEo45Z"},"source":["## Load Dataset"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"Fy8At_Mzo45a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded dataset: 727 original samples\n"]}],"source":["def load_json(filepath: str) -> list:\n","    \"\"\"\n","    Loads a JSON file and returns a list of dictionaries.\n","\n","    Args:\n","        filepath (str): Path to the JSON file.\n","\n","    Returns:\n","        list: List of parsed JSON objects (dictionaries).\n","    \"\"\"\n","    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n","        return json.load(f)\n","\n","# File paths\n","DATA_ZRS = \" \"\n","DATA_COT_ICL = \" \"\n","DATA_COT = \" \"\n","DATA_ORIGINAL = \" \"\n","\n","# Load dataset\n","data_zrs = load_json(DATA_ZRS)\n","data_cot_icl = load_json(DATA_COT_ICL)\n","data_cot = load_json(DATA_COT)\n","data_original = load_json(DATA_ORIGINAL)\n","\n","print(f\"Loaded dataset: {len(data_original)} original samples\")"]},{"cell_type":"markdown","metadata":{"id":"nzCeYClpo45b"},"source":["## Preprocess Text Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bAu7N2_so45c"},"outputs":[],"source":["# Text Preprocessing Function\n","def clean_text(text: str, keep_pipe=False) -> str:\n","    \"\"\"\n","    Cleans input text based on the preprocessing requirements of the specific task.\n","\n","    Args:\n","        text (str): The input string to be cleaned.\n","        keep_pipe (bool): If True, retains the '|' character (used for original answers with multiple references).\n","\n","    Returns:\n","        str: The cleaned text, processed according to the specified task requirements.\n","    \"\"\"\n","    if isinstance(text, str):\n","        if keep_pipe:\n","            # Uncomment the following line if you need to remove all special characters except '|'\n","            text = re.sub(r'[^a-zA-Z0-9ㄱ-ㅎㅏ-ㅣ가-힣| ]', '', text)\n","            pass\n","        else:\n","            # Uncomment the following line if you need to remove all special characters\n","            text = re.sub(r'[^a-zA-Z0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', text)\n","            pass\n","\n","        # Uncomment the following line if case normalization is required (for English text tasks)\n","        #text = text.lower()\n","\n","        # This step is always applied: Removes spaces from the text\n","        text = text.replace(\" \", \"\")\n","    return text\n","\n","# Convert to DataFrame (Min-Length Matching)\n","min_length = min(len(data_original), len(data_zrs), len(data_cot_icl), len(data_cot))\n","\n","if \"kor_consonant_vowel_combination\" in DATA_ORIGINAL:\n","    df = pd.DataFrame({\n","        \"original\": [item[\"words\"] for item in data_original[:min_length]],\n","        \"transformed\": [item[\"random\"] for item in data_original[:min_length]],\n","        \"zrs_answer\": [item[\"Answer\"] for item in data_zrs[:min_length]],\n","        \"cot_answer\": [item[\"Answer\"] for item in data_cot[:min_length]],\n","        \"cot_icl_answer\": [item[\"Answer\"] for item in data_cot_icl[:min_length]]\n","    })\n","else:\n","    df = pd.DataFrame({\n","        \"original\": [item[\"original\"] for item in data_original[:min_length]],\n","        \"transformed\": [item[\"transformed\"] for item in data_original[:min_length]],\n","        \"zrs_answer\": [item[\"Answer\"] for item in data_zrs[:min_length]],\n","        \"cot_answer\": [item[\"Answer\"] for item in data_cot[:min_length]],\n","        \"cot_icl_answer\": [item[\"Answer\"] for item in data_cot_icl[:min_length]]\n","    })\n","\n","# Apply Text Preprocessing\n","if \"kor_abbreviations\" in DATA_ORIGINAL:\n","    df[\"original\"] = df[\"original\"].apply(lambda x: clean_text(x, keep_pipe=True))\n","else:\n","    df[\"original\"] = df[\"original\"].apply(lambda x: clean_text(x, keep_pipe=False))\n","\n","df[[\"transformed\", \"zrs_answer\", \"cot_answer\", \"cot_icl_answer\"]] = \\\n","    df[[\"transformed\", \"zrs_answer\", \"cot_answer\", \"cot_icl_answer\"]].applymap(lambda x: clean_text(x, keep_pipe=False))\n","\n","df = df.fillna('').applymap(clean_text)\n","print(\"Text preprocessing completed.\")"]},{"cell_type":"markdown","metadata":{"id":"5rkazSHJo45d"},"source":["## Load Evaluation Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eGWS7zQno45d"},"outputs":[],"source":["# Determine FastText Model Based on File Name\n","filename = os.path.basename(DATA_ORIGINAL)  # Extract filename from path\n","\n","if filename.startswith(\"kor_\"):\n","    model_path = \"cc.ko.300.bin\"\n","elif filename.startswith(\"eng_\"):\n","    model_path = \"cc.en.300.bin\"\n","else:\n","    raise ValueError(\"Invalid file name! Must start with 'kor_' or 'eng_'.\")\n","\n","ko_model = models.fasttext.load_facebook_model(model_path)\n","print(f\"FastText model loaded: {model_path}\")"]},{"cell_type":"markdown","metadata":{"id":"1KczedHYo45d"},"source":["## Define Vectorization & Similarity Functions"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"1gsiCKKro45e"},"outputs":[],"source":["# Define Vectorization & Similarity Functions\n","def get_sentence_vector(sentence: str, model) -> np.ndarray:\n","    \"\"\"\n","    Computes the sentence vector using a pre-trained FastText model.\n","\n","    Args:\n","        sentence (str): The input sentence.\n","        model: FastText model for generating word embeddings.\n","\n","    Returns:\n","        np.ndarray: The averaged word vectors for the sentence. If no valid tokens exist, returns a zero vector.\n","    \"\"\"\n","    tokens = sentence.split()\n","    valid_tokens = [token for token in tokens if token in model.wv.key_to_index]\n","    return np.mean([model.wv[token] for token in valid_tokens], axis=0) if valid_tokens else np.zeros(model.vector_size)\n","\n","def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n","    \"\"\"\n","    Computes cosine similarity between two vectors.\n","\n","    Args:\n","        vec1 (np.ndarray): First vector.\n","        vec2 (np.ndarray): Second vector.\n","\n","    Returns:\n","        float: Cosine similarity score. Returns -1 if either vector has zero norm.\n","    \"\"\"\n","    norm1, norm2 = np.linalg.norm(vec1), np.linalg.norm(vec2)\n","    return np.dot(vec1, vec2) / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else -1.0\n"]},{"cell_type":"markdown","metadata":{"id":"xxXJzOo3o45e"},"source":["## Evaluate Models and Compute Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U8fwwxHyo45e"},"outputs":[],"source":["# Model Evaluation\n","overall_results = {}\n","\n","for model in [\"zrs\", \"cot\", \"cot_icl\"]:\n","    df[f\"{model}_accuracy\"] = [\n","        100 if row[f\"{model}_answer\"] in row[\"original\"].split(\"|\") else 0\n","        for _, row in df.iterrows()\n","    ]\n","    df[f\"{model}_edit_distance\"] = [\n","        min(levenshtein(row[f\"{model}_answer\"], gt) for gt in row[\"original\"].split(\"|\"))\n","        for _, row in df.iterrows()\n","    ]\n","    df[f\"{model}_cosine_similarity\"] = [\n","        max(cosine_similarity(get_sentence_vector(row[f\"{model}_answer\"], ko_model), get_sentence_vector(gt, ko_model))\n","            for gt in row[\"original\"].split(\"|\"))\n","        for _, row in df.iterrows()\n","    ]\n","\n","    # Store overall evaluation metrics\n","    overall_results[model.upper()] = {\n","        \"Accuracy\": np.mean(df[f\"{model}_accuracy\"]),\n","        \"Edit Distance\": np.mean(df[f\"{model}_edit_distance\"]),\n","        \"Cosine Similarity\": np.mean(df[f\"{model}_cosine_similarity\"])\n","    }\n","\n","print(\"Evaluation completed.\")"]},{"cell_type":"markdown","metadata":{"id":"YTmX2UK1o45f"},"source":["## Display Final Performance Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oKD_zEkxo45f"},"outputs":[],"source":["df_performance = pd.DataFrame.from_dict(overall_results, orient='index')\n","df_performance.rename(columns={'Accuracy': 'Accuracy(%)', 'Edit Distance': 'Edit Distance', 'Cosine Similarity': 'Cosine Similarity'}, inplace=True)\n","\n","print(\"Overall Model Performance\")\n","df_performance"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"ds_study","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
