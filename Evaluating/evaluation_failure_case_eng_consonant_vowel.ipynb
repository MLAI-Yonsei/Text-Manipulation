{"cells":[{"cell_type":"markdown","metadata":{"id":"I9CRBArXw5dG"},"source":["## Fuction Definitions for failure case"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"},"id":"RgkqNvc3w5dI"},"outputs":[],"source":["import pandas as pd\n","import json\n","import numpy as np\n","from nltk.corpus import words as nltk_words\n","from Levenshtein import distance as levenshtein\n","from gensim import models\n","import nltk\n","import re\n","\n","def load_json(filepath):\n","    \"\"\"\n","    Loads a JSON file and returns a list of dictionaries.\n","\n","    Args:\n","        filepath (str): Path to the JSON file.\n","\n","    Returns:\n","        list: List of parsed JSON objects (dictionaries).\n","    \"\"\"\n","    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n","        return json.load(f)\n","\n","\n","# Load nltk word list(noun)\n","def load_english_words():\n","    \"\"\"\n","    Downloads and loads a set of English words from the nltk corpus.\n","\n","    Returns:\n","        set: A set of valid English words.\n","    \"\"\"\n","    nltk.download('words')\n","    return set(nltk_words.words())\n","\n","\n","ENGLISH_WORDS = load_english_words()\n","\n","\n","def is_english_word(word):\n","    \"\"\"\n","    Checks whether a given word exists in the loaded English word list.\n","\n","    Args:\n","        word (str): The word to be checked.\n","\n","    Returns:\n","        bool: True if the word exists in the English dictionary, False otherwise.\n","    \"\"\"\n","    return word in ENGLISH_WORDS\n","\n","\n","# Alphabet construction and word validation functions\n","def validate_word(correct_word, user_word):\n","    \"\"\"\n","    Validates whether the user's word is a valid anagram of the correct word\n","    or exactly matches the correct word.\n","\n","    Args:\n","        correct_word (str): The original correct word.\n","        user_word (str): The word provided by the user.\n","\n","    Returns:\n","        str: The validated user word if correct, otherwise an empty string.\n","    \"\"\"\n","    correct_word = clean_text(correct_word)\n","    user_word = clean_text(user_word)\n","\n","    if sorted(correct_word) == sorted(user_word) and is_english_word(user_word):\n","        return user_word\n","    elif correct_word == user_word:\n","        return user_word\n","    else:\n","        return \"\"\n","\n","\n","# Clean text\n","def clean_text(text):\n","    \"\"\"\n","    Cleans input text based on the preprocessing requirements of the specific task.\n","\n","    Args:\n","        text (str): The input string to be cleaned.\n","        keep_pipe (bool): If True, retains the '|' character (used for original answers with multiple references).\n","\n","    Returns:\n","        str: The cleaned text, processed according to the specified task requirements.\n","    \"\"\"\n","    if isinstance(text, str):\n","        text = text.lower()\n","        text = re.sub(r'[^a-zA-Z0-9ㄱ-ㅎㅏ-ㅣ가-힣]', '', text)\n","    return text\n","\n","## Define Vectorization & Similarity Functions\n","# Sentence vector calculation functions\n","def get_sentence_vector(sentence, model):\n","    \"\"\"\n","    Computes the sentence vector using a pre-trained FastText model.\n","\n","    Args:\n","        sentence (str): The input sentence.\n","        model: FastText model for generating word embeddings.\n","\n","    Returns:\n","        np.ndarray: The averaged word vectors for the sentence. If no valid tokens exist, returns a zero vector.\n","    \"\"\"\n","    tokens = sentence.split()\n","    valid_tokens = [token for token in tokens if token in model.wv.key_to_index]\n","    return np.mean([model.wv[token] for token in valid_tokens], axis=0) if valid_tokens else np.zeros(model.vector_size)\n","\n","\n","# Cosine similarity calculation function\n","def cosine_similarity(vec1, vec2):\n","    \"\"\"\n","    Computes cosine similarity between two vectors.\n","\n","    Args:\n","        vec1 (np.ndarray): First vector.\n","        vec2 (np.ndarray): Second vector.\n","\n","    Returns:\n","        float: Cosine similarity score. Returns -1 if either vector has zero norm.\n","    \"\"\"\n","    norm1, norm2 = np.linalg.norm(vec1), np.linalg.norm(vec2)\n","    return np.dot(vec1, vec2) / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else -1.0\n","\n","\n","## Evaluate Models and Compute Metrics\n","# Calculate model-specific performance metrics\n","def evaluate_model_performance(df, models, ft_model):\n","    overall_results = {}\n","\n","    for model in models:\n","        # Accuracy\n","        df[f\"{model}_accuracy\"] = [\n","            100 if validate_word(row[f\"{model}\"], row[\"original\"]) else 0\n","            for _, row in df.iterrows()\n","        ]\n","\n","        # Edit distance\n","        df[f\"{model}_edit_distance\"] = [\n","            levenshtein(row[f\"{model}\"], row[\"original\"])\n","            for _, row in df.iterrows()\n","        ]\n","\n","        # Cosine Similarity\n","        df[f\"{model}_cosine_similarity\"] = [\n","            cosine_similarity(\n","                get_sentence_vector(row[f\"{model}\"], ft_model),\n","                get_sentence_vector(row[\"original\"], ft_model)\n","            )\n","            for _, row in df.iterrows()\n","        ]\n","\n","        # Overall performance evaluation by model\n","        overall_results[model.upper()] = {\n","            \"Accuracy\": np.mean(df[f\"{model}_accuracy\"]),\n","            \"Edit Distance\": np.mean(df[f\"{model}_edit_distance\"]),\n","            \"Cosine Similarity\": np.mean(df[f\"{model}_cosine_similarity\"])\n","        }\n","\n","    # Store overall evaluation metrics\n","    df_performance = pd.DataFrame.from_dict(overall_results, orient='index')\n","\n","    return df_performance"]},{"cell_type":"markdown","metadata":{"id":"ljoXabaOw5dK"},"source":["## Load Evaluation Model(Fasttext_eng)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"},"id":"t8GTGGu5w5dK"},"outputs":[],"source":["# Load Fasttext model\n","ft_model = models.fasttext.load_facebook_model('cc.en.300.bin')"]},{"cell_type":"markdown","metadata":{"id":"kMgWXyPmw5dK"},"source":["## Load files(each model)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"},"id":"-hAcil7cw5dK"},"outputs":[],"source":["# Load files of each model\n","gpt4o = load_json(\" \")\n","gemini = load_json(\" \")\n","claude = load_json(\" \")\n","gpto3 = load_json(\" \")\n","data_original = load_json(\" \")  # answer(original) data\n","\n","# Adjust Dataset to Minimum Length\n","min_length = min(len(data_original), len(gpt4o), len(gemini), len(claude), len(gpto3))\n","\n","# transform to Dataframe\n","df = pd.DataFrame({\n","    \"original\": [item[\"original\"] for item in data_original[:min_length]],\n","    \"transformed\": [item[\"transformed\"] for item in data_original[:min_length]],\n","    \"gpt4o\": [item[\"Answer\"] for item in gpt4o[:min_length]],\n","    \"gemini\": [item[\"Answer\"] for item in gemini[:min_length]],\n","    \"claude\": [item[\"Answer\"] for item in claude[:min_length]],\n","    \"gpto3\": [item[\"Answer\"] for item in gpto3[:min_length]],\n","})\n","\n","# apply \"clean_text\"\n","df = df.fillna('').applymap(clean_text)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"Eb09Ox0Qw5dL"},"source":["## Display Final Performance Results"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"},"id":"SAVSpJruw5dL"},"outputs":[],"source":["# evaluate each model\n","models = [\"gpt4o\", \"gemini\", \"claude\", \"gpto3\"]\n","df_performance = evaluate_model_performance(df, models, ft_model)\n","\n","print(df_performance)"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}